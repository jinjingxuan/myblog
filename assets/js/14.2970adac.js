(window.webpackJsonp=window.webpackJsonp||[]).push([[14],{345:function(t,s,a){t.exports=a.p+"assets/img/ghost.5aac3e85.png"},346:function(t,s,a){t.exports=a.p+"assets/img/ghost2.9ad5735d.png"},347:function(t,s,a){t.exports=a.p+"assets/img/ghost3.c75bbcb0.png"},348:function(t,s,a){t.exports=a.p+"assets/img/ghost4.82289519.png"},349:function(t,s,a){t.exports=a.p+"assets/img/ghost5.30918901.jpg"},350:function(t,s,a){t.exports=a.p+"assets/img/ghost6.f1c45cdf.png"},351:function(t,s,a){t.exports=a.p+"assets/img/ghost7.96577e4d.png"},352:function(t,s,a){t.exports=a.p+"assets/img/dh.8139a271.jpg"},353:function(t,s,a){t.exports=a.p+"assets/img/dh2.c4dbe2db.jpg"},354:function(t,s,a){t.exports=a.p+"assets/img/dh3.4442c2d6.jpg"},355:function(t,s,a){t.exports=a.p+"assets/img/dh4.2664c4e1.jpg"},356:function(t,s,a){t.exports=a.p+"assets/img/dh5.0e7ecf72.jpg"},357:function(t,s,a){t.exports=a.p+"assets/img/dh6.ef61e55a.jpg"},565:function(t,s,a){"use strict";a.r(s);var n=a(14),e=Object(n.a)({},(function(){var t=this,s=t._self._c;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"北大肖臻老师《区块链技术与应用》公开课学习-6"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#北大肖臻老师《区块链技术与应用》公开课学习-6"}},[t._v("#")]),t._v(" 北大肖臻老师《区块链技术与应用》公开课学习 6")]),t._v(" "),s("ul",[s("li",[t._v("学习地址："),s("a",{attrs:{href:"https://www.bilibili.com/video/BV1Vt411X7JF",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://www.bilibili.com/video/BV1Vt411X7JF"),s("OutboundLink")],1)]),t._v(" "),s("li",[t._v("参考文章："),s("a",{attrs:{href:"https://blog.nowcoder.net/n/6d1f8b54058347d4a217b961e5b965f1",target:"_blank",rel:"noopener noreferrer"}},[t._v("北京大学肖臻老师《区块链技术与应用》公开课笔记20——ETH中GHOST协议篇"),s("OutboundLink")],1)])]),t._v(" "),s("h2",{attrs:{id:"ghost-协议"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#ghost-协议"}},[t._v("#")]),t._v(" Ghost 协议")]),t._v(" "),s("p",[s("code",[t._v("BTC")]),t._v(" 系统中出块时间为 "),s("code",[t._v("10min")]),t._v("，而以太坊中出块时间被降低到 "),s("code",[t._v("15s")]),t._v("左右，虽然有效提高了系统反应时间和吞吐率，却也导致系统临时性分叉变成常态，且分叉数目更多。这对于共识协议来说，就存在很大挑战。在 "),s("code",[t._v("BTC")]),t._v(" 系统中，不在最长合法链上的节点最后都是作废的，但如果在以太坊系统中，如果这样处理，由于系统中经常性会出现分叉，则矿工挖到矿很大可能会被废弃，这会大大降低矿工挖矿积极性。而对于个人矿工来说，和大型矿池相比更是存在天然劣势。\n对此，以太坊设计了新的共识协议—— "),s("code",[t._v("GHOST协议")]),t._v(" (该协议并非原创，而是对原本就有的Ghost协议进行了改进)。")]),t._v(" "),s("p",[s("img",{attrs:{src:a(345),alt:""}})]),t._v(" "),s("blockquote",[s("p",[t._v("如图，假定以太坊系统存在以下情况，"),s("code",[t._v("A、B、C、D")]),t._v(" 在四个分支上，最后，随着时间推移 "),s("code",[t._v("B")]),t._v(" 所在链成为最长合法链，因此 "),s("code",[t._v("A、C、D")]),t._v(" 区块都作废，但为了补偿这些区块所属矿工所作的工作，给这些区块一些 "),s("code",[t._v("补偿")]),t._v("，并称其为 "),s("code",[t._v("Uncle Block")]),t._v("（叔父区块）。\n规定 "),s("code",[t._v("E")]),t._v(" 区块在发布时可以将 "),s("code",[t._v("A、C、D")]),t._v(" 叔父区块包含进来，"),s("code",[t._v("A、C、D")]),t._v(" 叔父区块可以得到出块奖励的 "),s("code",[t._v("7/8")]),t._v("，而为了激励 "),s("code",[t._v("E")]),t._v(" 包含叔父区块，规定 "),s("code",[t._v("E")]),t._v(" 每包含一个叔父区块可以额外得到 "),s("code",[t._v("1/32")]),t._v(" 的出块奖励。为了防止 "),s("code",[t._v("E")]),t._v(" 大量包含叔父区块，规定一个区块只能最多包含 "),s("code",[t._v("2")]),t._v(" 个叔父区块，因此 "),s("code",[t._v("E")]),t._v(" 在 "),s("code",[t._v("A、C、D")]),t._v(" 中最多只能包含 "),s("code",[t._v("2")]),t._v(" 个区块作为自己的出块奖励。\n"),s("code",[t._v("E")]),t._v(" 把 "),s("code",[t._v("A")]),t._v(" 作为叔父区块的前提是：在挖 "),s("code",[t._v("E")]),t._v(" 这个区块的时候就知道 "),s("code",[t._v("A")]),t._v(" 的存在了，把 "),s("code",[t._v("A")]),t._v(" 写在自己的块头里修改后继续挖，因为挖矿是无记忆性的所以这并不影响什么。")])]),t._v(" "),s("p",[t._v("问题：")]),t._v(" "),s("ol",[s("li",[t._v("因为叔父区块最多只能包含 2 个，如果出现 3 个怎么办？")]),t._v(" "),s("li",[t._v("矿工自私，故意不包含叔父区块，导致叔父区块 7/8 出块奖励没了，而自己仅仅损失 1/32。如果甲、乙两个大型矿池存在竞争关系，那么他们可以采用故意不包含对方的叔父区块，因为这样对自己损失小而对对方损失大。")])]),t._v(" "),s("p",[s("img",{attrs:{src:a(346),alt:""}})]),t._v(" "),s("blockquote",[s("p",[t._v("如图1为对上面例子的补充，"),s("code",[t._v("F")]),t._v(" 为 "),s("code",[t._v("E")]),t._v(" 后面一个新的区块。因为规定 "),s("code",[t._v("E")]),t._v(" 最多只能包含 "),s("code",[t._v("2")]),t._v(" 个叔父区块，所以假定 "),s("code",[t._v("E")]),t._v(" 包含了 "),s("code",[t._v("C")]),t._v(" 和 "),s("code",[t._v("D")]),t._v("。此时，"),s("code",[t._v("F")]),t._v(" 也可以将 "),s("code",[t._v("A")]),t._v(" 认为自己的的叔父区块(实际上并非叔父辈的，而是爷爷辈的)。如果继续往下挖，"),s("code",[t._v("F")]),t._v(" 后的新区块仍然可以包含 "),s("code",[t._v("B")]),t._v(" 同辈的区块。这样，就有效地解决了上面的问题。\n就算自己挖的区块成为了叔父区块，自己也可以在最长合法链上挖，然后把自己包含进去。")])]),t._v(" "),s("p",[t._v("问题："),s("code",[t._v("叔父")]),t._v(" 最多可以隔多少代？")]),t._v(" "),s("p",[s("img",{attrs:{src:a(347),alt:""}})]),t._v(" "),s("blockquote",[s("p",[t._v("以太坊中规定，如果 "),s("code",[t._v("M")]),t._v(" 包含 "),s("code",[t._v("F")]),t._v(" 区块，则 "),s("code",[t._v("F")]),t._v(" 获得 "),s("code",[t._v("7/8")]),t._v(" 出块奖励；如果 "),s("code",[t._v("M")]),t._v(" 包含 "),s("code",[t._v("E")]),t._v(" 区块，则 "),s("code",[t._v("F")]),t._v(" 获得 "),s("code",[t._v("6/8")]),t._v(" 出块奖励，以此类推向前。直到包含 "),s("code",[t._v("A")]),t._v(" 区块，A获得 "),s("code",[t._v("2/8")]),t._v(" 出块奖励，再往前的叔父区块，对于 "),s("code",[t._v("M")]),t._v(" 来说就不再认可其为 "),s("code",[t._v("M")]),t._v(" 的叔父了(合法的叔父只有 6 辈)。对于 "),s("code",[t._v("M")]),t._v(" 来说，无论包含哪个辈分的叔父，得到的出块奖励都是 "),s("code",[t._v("1/32")]),t._v(" 出块奖励。\n这样，就方便了全节点进行记录（有效叔父区块不会无限增加），此外，也从协议上鼓励一旦出现分叉马上进行合并。")])]),t._v(" "),s("p",[t._v("以上这些都是为了解决临时性分叉的问题，那么为什么比特币和以太坊要设计最长合法链呢，是为了防止数据被篡改。")]),t._v(" "),s("ul",[s("li",[t._v("BTC 奖励：block reward（静态奖励）+ tx fee（动态奖励）")]),t._v(" "),s("li",[t._v("ETH 奖励：block reward（静态奖励）+ gas fee（动态奖励，叔父区块是得不到的）")]),t._v(" "),s("li",[t._v("BTC 中为了人为制造稀缺性，比特币每隔一段时间出块奖励会降低，最终当出块奖励趋于 0 后会主要依赖于交易费运作。而以太坊中并没有人为规定每隔一段时间降低出块奖励。")])]),t._v(" "),s("blockquote",[s("p",[t._v("如果区块里包含智能合约，执行智能合约的时候会获得汽油费。但是所占的比例很小，跟比特币类似交易费占比很小。")])]),t._v(" "),s("p",[t._v("把叔父区块包含进来的时候，叔父区块里的交易要不要执行？")]),t._v(" "),s("blockquote",[s("p",[t._v("不应该，叔父区块和同辈的主链上区块有可能包含有冲突的交易。执行完父区块的交易再去执行叔父区块的交易可能就变成非法的了。因此，一个节点在收到一个叔父区块的时候，只检查区块合法性而不检查其中交易的合法性。")])]),t._v(" "),s("p",[t._v("分叉后的区块后面还跟着其他区块怎么处理？\n"),s("img",{attrs:{src:a(348),alt:""}})]),t._v(" "),s("blockquote",[s("p",[s("code",[t._v("A->F")]),t._v(" 该链并非一个最长合法链，所以 "),s("code",[t._v("B->F")]),t._v(" 这些区块怎么办？可不可以规定将下面整条链作为一个整体，给予出块奖励？\n不行，这一定程度上鼓励了分叉攻击(如果 A 转给 B 一大笔钱，B 等了很多区块后觉得没什么问题了，此时 A 发动分叉攻击将钱转给 A'，这需要挖出更多的区块代价是很大的，但是，后面的区块都给奖励，这就降低了分叉攻击的成本，因为即使攻击失败也有奖励)。因此，"),s("code",[t._v("ETH")]),t._v(" 系统中规定，只认可 "),s("code",[t._v("A")]),t._v(" 区块为叔父区块，给予其补偿，而其后的区块全部作废。")])]),t._v(" "),s("h3",{attrs:{id:"具体例子"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#具体例子"}},[t._v("#")]),t._v(" 具体例子")]),t._v(" "),s("p",[s("a",{attrs:{href:"https://etherscan.io/",target:"_blank",rel:"noopener noreferrer"}},[t._v("https://etherscan.io/"),s("OutboundLink")],1)]),t._v(" "),s("p",[s("img",{attrs:{src:a(349),alt:""}})]),t._v(" "),s("ul",[s("li",[t._v("Block Height 为当前区块的序号，UncleNumber 为叔父区块的序号")]),t._v(" "),s("li",[t._v("如果相差为 1，说明是刚刚相差一辈，获得 "),s("code",[t._v("7/8")]),t._v(" 的奖励，以此类推")])]),t._v(" "),s("p",[s("img",{attrs:{src:a(350),alt:""}})]),t._v(" "),s("ul",[s("li",[t._v("Block Reward: 出块奖励 + 汽油费 + 包含叔父区块奖励（1个, 0.09375 = 3 * 1 / 32）")]),t._v(" "),s("li",[t._v("Uncles Reward: 叔父区块获得的奖励，通过计算可知隔两代，2.25 = 3 * 6 / 8")])]),t._v(" "),s("p",[s("img",{attrs:{src:a(351),alt:""}})]),t._v(" "),s("ul",[s("li",[t._v("Block Reward: 出块奖励 + 汽油费 + 包含叔父区块奖励（2个, 0.1875 = 3 * 2 * 1 / 32）")]),t._v(" "),s("li",[t._v("Uncles Reward: 叔父区块获得的奖励，通过计算可知分别隔两代和隔一代，4.875 = 2.25 (3 * 6 / 8) + 2.625 (3 * 7 / 8)")])]),t._v(" "),s("h2",{attrs:{id:"eth-挖矿算法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#eth-挖矿算法"}},[t._v("#")]),t._v(" ETH 挖矿算法")]),t._v(" "),s("p",[s("strong",[t._v("Block chain is secured by mining.")]),t._v("\n比特币是天然的 "),s("code",[t._v("bug bounty")]),t._v("，只要你能找到挖矿的 bug，就能获得赏金。然而比特币的缺点在于专业矿机的出现，不是很符合去中心化的概念。中本聪写到 "),s("code",[t._v("one cpu, one vote")]),t._v("，希望每个人都能参与进去。所以后来的加密货币在设计挖矿算法的时候，希望 "),s("code",[t._v("ASIC resistance")]),t._v("，怎么解决呢？")]),t._v(" "),s("ul",[s("li",[t._v("memory hard mining puzzle: 增加对内存的访问, 因为 "),s("code",[t._v("ASIC")]),t._v(" 主要是算力强，但是内存访问能力弱。一个例子就是 "),s("code",[t._v("LiteCoin")]),t._v("，它是基于 "),s("code",[t._v("scrypt")]),t._v(" 对于内存要求较高，在内存中创建了一个比较大的数组进行运算。")])]),t._v(" "),s("p",[t._v("好的 "),s("code",[t._v("puzzle")]),t._v(" 的准则是 "),s("code",[t._v("diffculty to solve, but easy to verify")]),t._v("，但是上述算法在验证的时候也需要同样大小的数组，对于轻节点不友好，如果是手机上的 "),s("code",[t._v("APP")]),t._v(" 则很难实现，所以实际上莱特币设计的数组大小为 "),s("code",[t._v("128K")]),t._v("。但是这种理念对于莱特币的 "),s("strong",[t._v("冷启动")]),t._v(" 很有意义。")]),t._v(" "),s("p",[t._v("ETH 中设计了两个数据集："),s("code",[t._v("16M cache")]),t._v(" 和 "),s("code",[t._v("1G dataset")]),t._v(" ("),s("code",[t._v("DAG")]),t._v(")。")]),t._v(" "),s("blockquote",[s("p",[t._v("16M cache 生成方式：通过 Seed (种子数)进行一些运算获得第一个数，之后每个数字都是通过前一个位置的值取哈希获得的。\n1G DAG 生成方式：从小数组中按照伪随机顺序读取一些元素，如第一次读取 A 位置数据，对当前哈希值更新迭代算出下一次读取位置 B，再进行哈希值更新迭代计算出 C 位置元素。如此来回迭代读取 256 次，最终算出一个数作为 DAG 中第一个元素。\n考虑到计算机内存不断增大，因此该两个数组需要定期增大。\n轻节点只保存小的 cache，验证时进行计算即可。但对于挖矿来说，如果这样则大部分算力都花费在了通过 Cache 计算 DAG 上面，因此，其必须保存大的数组 DAG 以便于更快挖矿。")])]),t._v(" "),s("p",[t._v("以太坊挖矿过程：")]),t._v(" "),s("blockquote",[s("p",[t._v("根据区块 "),s("code",[t._v("block header")]),t._v(" 和其中的 "),s("code",[t._v("Nonce")]),t._v(" 值计算一个初始哈希，在 "),s("code",[t._v("DAG")]),t._v(" 上根据其映射到某个初始位置 "),s("code",[t._v("A1")]),t._v("，读取 "),s("code",[t._v("A1")]),t._v(" 位置的数及其相邻的后一个位置 "),s("code",[t._v("A2")]),t._v(" 上的数，根据该两个数进行运算，算得下一个位置 "),s("code",[t._v("B1")]),t._v("，读取 "),s("code",[t._v("B1")]),t._v(" 和 "),s("code",[t._v("B2")]),t._v(" 位置上的数，依次类推，迭代读取 "),s("code",[t._v("64")]),t._v(" 次，共读取 "),s("code",[t._v("128")]),t._v(" 个数。最后，计算出一个哈希值与挖矿难度目标阈值比较，若不符合就更换 "),s("code",[t._v("Nonce")]),t._v("，重复以上操作直到最终计算哈希值符合难度要求。")])]),t._v(" "),s("p",[t._v("每隔 30000 个块会重新生成 seed （对原来的 seed 求哈希），并且利用新的 seed 生成新的 cache，cache 的初始大小为 16M，每隔 30000 个块重新生成时增大初始大小的 1/128 - 128K，通过 seed 计算 cache 的伪代码：")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("mkcache")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cache_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" seed"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  o "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hash")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("seed"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("append"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hash")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" o"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(";")]),t._v("\n")])])]),s("p",[t._v("DAG 初始大小是 1G，也是每隔 30000 个块更新，同时增大初始大小的 1/128 - 8M，通过 cache 来生成 dataset 中第 i 个元素的伪代码")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("calc_dataset_item")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  cache_size "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" cache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("size\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 通过 cache 中第 i 个元素生成 mix")]),t._v("\n  mix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hash")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("cache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("i "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" cache_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("^")]),t._v(" i"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 循环 256 次")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" j "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("256")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 每次通过 get_int_from_item 根据当前的 mix 求得下一个 cache 元素的下标")]),t._v("\n    cache_index "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_int_from_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    mix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" cache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("cache_index "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" cache_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 最终返回 mix 的哈希值，得到第 i 个 dataset 中的元素")]),t._v("\n   "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hash")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("挖矿过程与轻节点验证过程，先通过 header 和 nonce 求出一个初始的 mix，然后进行 64 次循环，根据 mix 求出要访问的 dataset 的元素下标，然后根据这个下标访问 dataset 中两个连续的值。")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("hashimoto_full")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("header"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nonce"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" full_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  mix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hash")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("header"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nonce"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    dataset_index "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_int_from_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" full_size\n    mix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dataset_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    mix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("dataset_index "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hash")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 轻节点是临时计算出用到的 dataset 元素，而矿工是直接访存")]),t._v("\n"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("hashimoto_light")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("header"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nonce"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" full_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  mix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hash")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("header"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nonce"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("for")]),t._v(" i "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("in")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("range")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    dataset_index "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" get_int_from_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" full_size\n    mix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" calc_dataset_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("cache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset_index"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n    mix "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" make_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" calc_dataset_item"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),t._v("cache"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset_index "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("hash")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("mix"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])])]),s("p",[t._v("挖矿伪代码，随机初始化 nonce，再一个个重试 nonce")]),t._v(" "),s("div",{staticClass:"language-python extra-class"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("def")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("mine")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("full_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" header"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" target"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n  nonce "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" random"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("randint"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("while")]),t._v(" hashimoto_full"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("header"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" nonce"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" full_size"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" dataset"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" target"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(":")]),t._v("\n    nonce "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("nonce "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("+")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("%")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2")]),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("**")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("64")]),t._v("\n  "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" nonce\n")])])]),s("blockquote",[s("p",[t._v("目前以太坊挖矿以 GPU 为主，可见其设计较为成功，这与以太坊设计的挖矿算法 (Ethash) 所需要的大内存具有很大关系。\n1G的大数组与128k相比，差距8000多倍，即使是16MB与128K相比，也大了一百多倍，可见对内存需求的差距很大，况且两个数组大小是会不断增长的。")]),t._v(" "),s("p",[t._v("当然，以太坊实现 "),s("code",[t._v("ASIC Resistance")]),t._v(" 除了挖矿算法设计之外，还存在另外一个原因，即其预期从"),s("strong",[t._v("工作量证明(proof of work)"),s("strong",[t._v("转向")]),t._v("权益证明(proof of stake)")])]),t._v(" "),s("p",[t._v("而这对于 ASIC 厂商来说有威胁。因为 ASIC 芯片研发周期很长，成本很高，如果以太坊转入权益证明，这些投入的研发费用将全部白费，但截至目前，以太坊仍然基于 POW 共识机制。")])]),t._v(" "),s("h3",{attrs:{id:"预挖矿-pre-mining"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#预挖矿-pre-mining"}},[t._v("#")]),t._v(" 预挖矿(pre mining)")]),t._v(" "),s("p",[t._v("以太坊中采用的预挖矿的机制。这里 "),s("strong",[t._v("预挖矿")]),t._v(" 并不挖矿，而是在开发以太坊时，给开发者预留了一部分货币，其实这部分预留了很多(在 Genesis 创世纪块)。\n和 "),s("code",[t._v("Pre-Mining")]),t._v(" 对应，还有 "),s("code",[t._v("Pre-Sale")]),t._v("，"),s("code",[t._v("Pre-Sale")]),t._v(" 指的是将预留的货币出售掉用于后续开发工作（众筹）")]),t._v(" "),s("blockquote",[s("p",[t._v("挖矿算法设计一直趋向于让大众参与，这才是公平的。但也有人认为让普通计算机参与挖矿是不安全的，像比特币那样，让中心化矿池参与挖矿才是安全的。为什么呢？\n因为要攻击系统，需要购入大量矿机通过算力进行 51% 攻击，而且这种矿机不能用于其他币种。而且攻击成功后，证明安全性不够必然导致该币的价值跳水，攻击者投入的硬件成本将会全部打水漂。相反让通用计算机也参与挖矿，发动攻击成本便大幅度降低，目前的大型互联网公司，将其服务器聚集起来进行攻击即可，而攻击完成后这些服务器仍然可以转而运行日常业务。因此有人认为在挖矿上面，"),s("code",[t._v("ASIC")]),t._v(" 矿机一统天下才是最安全的方式。")])]),t._v(" "),s("h2",{attrs:{id:"以太坊挖矿难度调整"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#以太坊挖矿难度调整"}},[t._v("#")]),t._v(" 以太坊挖矿难度调整")]),t._v(" "),s("p",[s("img",{attrs:{src:a(352),alt:""}})]),t._v(" "),s("p",[s("img",{attrs:{src:a(353),alt:""}})]),t._v(" "),s("p",[s("img",{attrs:{src:a(354),alt:""}})]),t._v(" "),s("h3",{attrs:{id:"难度炸弹"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#难度炸弹"}},[t._v("#")]),t._v(" 难度炸弹")]),t._v(" "),s("p",[t._v("以太坊在设计之初就计划要逐步从 "),s("code",[t._v("POW")]),t._v("（工作量证明）转向 "),s("code",[t._v("POS")]),t._v("（权益证明），而权益证明不需要挖矿。从旁观者角度来看，挖矿消耗了大量电力、资金等，如果转入放弃挖矿必然是一件好事。但从矿工的角度，花费了很大精力投入成本购买设备，突然被告知不挖矿了，这必然是一件很难接受的事情。因此，以太坊在设计之初便添加了难度炸弹，迫使矿工转入 "),s("code",[t._v("POS")]),t._v("。")]),t._v(" "),s("p",[s("img",{attrs:{src:a(355),alt:""}})]),t._v(" "),s("blockquote",[s("p",[t._v("难度炸弹属于指数级别，在以太坊早期时，区块号较小难度炸弹计算所得值较小，难度调整级别基本上通过难度调整中的自适应难度调整部分决定，而随着越来越多区块被挖出，难度炸弹的威力开始显露出来，这也就使得挖矿变得越来越难，从而迫使矿工愿意转入 "),s("code",[t._v("POS")]),t._v("。\n但是目前以太坊共识机制仍然是 "),s("code",[t._v("POW")]),t._v("，依然需要矿工参与挖矿维护以太坊系统的稳定。所以以太坊决定将区块号减去 300w 个（就是上面图中的公示）")])]),t._v(" "),s("p",[s("img",{attrs:{src:a(356),alt:""}})]),t._v(" "),s("blockquote",[s("p",[t._v("上图中展示了回退 300w 个区块前后的难度炸弹的为例")])]),t._v(" "),s("h3",{attrs:{id:"以太坊发展的四个阶段"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#以太坊发展的四个阶段"}},[t._v("#")]),t._v(" 以太坊发展的四个阶段")]),t._v(" "),s("p",[t._v("以太坊发展存在四个阶段，我们目前处于第三个阶段中的拜占庭阶段，难度炸弹回调就是在拜占庭阶段进行的。")]),t._v(" "),s("p",[s("img",{attrs:{src:a(357),alt:""}})]),t._v(" "),s("h3",{attrs:{id:"代码实现"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#代码实现"}},[t._v("#")]),t._v(" 代码实现")]),t._v(" "),s("div",{staticClass:"language-go extra-class"},[s("pre",{pre:!0,attrs:{class:"language-go"}},[s("code",[s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("func")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("calcDifficultyByzantium")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("curVersion "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("string")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" time "),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("uint64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" parent "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("types"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Header"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("big"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Int "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// https://github.com/MatrixAINetwork/EIPs/issues/100.")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// algorithm:")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// diff = (parent_diff +")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//         (parent_diff / 2048 * max((2 if len(parent.uncles) else 1) - ((timestamp - parent.timestamp) // 9), -99))")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//        ) + 2^(periodCount - 2)")]),t._v("\n\n\tbigTime "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("new")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Int"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("SetUint64")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 当前区块时间戳")]),t._v("\n\tbigParentTime "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("new")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Int"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Time"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 父区块时间戳")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// holds intermediate values to make the algo easier to read & audit")]),t._v("\n\tx "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("new")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Int"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\ty "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("new")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Int"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\tlogger "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":=")]),t._v(" log"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("New")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CalcDifficulty diff"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Difficulty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// (2 if len(parent_uncles) else 1) - (block_timestamp - parent_timestamp) // 9")]),t._v("\n\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Sub")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bigTime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" bigParentTime"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// 当前区块时间戳 - 父区块时间戳 = 出块时间")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("var")]),t._v(" durationLimit "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("*")]),t._v("big"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Int\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" manversion"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("VersionCmp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("curVersion"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" manversion"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("VersionGamma"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\tdurationLimit "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("VersionGammaDurationLimit\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\tdurationLimit "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DurationLimit\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\tlogger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Info")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"CalcDifficulty diff"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"duration"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" durationLimit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("String")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Div")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" durationLimit"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("UncleHash "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" types"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("EmptyUncleHash "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Sub")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("else")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Sub")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// max((2 if len(parent_uncles) else 1) - (block_timestamp - parent_timestamp) // 9, -99)")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Cmp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bigMinus99"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("bigMinus99"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// parent_diff + (parent_diff / 2048 * max((2 if len(parent.uncles) else 1) - ((timestamp - parent.timestamp) // 9), -99))")]),t._v("\n\ty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Div")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Difficulty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("DifficultyBoundDivisor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Sign")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("==")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\ty "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" big1\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Mul")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Add")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Difficulty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n  logger"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Info")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"cal Diff"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"x"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"y"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token string"}},[t._v('"minDiff"')]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MinimumDifficulty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// minimum difficulty can ever be (before exponential factor)")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Cmp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MinimumDifficulty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("<")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Set")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("params"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("MinimumDifficulty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// MinimumDifficulty 是难度下限 D0")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// calculate a fake block number for the ice-age delay:")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//   https://github.com/MatrixAINetwork/EIPs/pull/669")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("//   fake_block_number = min(0, block.number - 3_000_000")]),t._v("\n\tfakeBlockNumber "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("new")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Int"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Number"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Cmp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big2999999"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\tfakeBlockNumber "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" fakeBlockNumber"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Sub")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("parent"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("Number"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" big2999999"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// Note, parent is 1 less than the actual block number")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// for the exponential factor")]),t._v("\n\tperiodCount "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(":=")]),t._v(" fakeBlockNumber\n\tperiodCount"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Div")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("periodCount"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" expDiffPeriod"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v('// the exponential factor, commonly referred to as "the bomb"')]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("// diff = diff + 2^(periodCount - 2)")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("if")]),t._v(" periodCount"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Cmp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big1"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v(">")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("{")]),t._v("\n\t\ty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Sub")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("periodCount"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" big2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\ty"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Exp")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("big2"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("nil")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t\tx"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token function"}},[t._v("Add")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("x"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" y"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n\t"),s("span",{pre:!0,attrs:{class:"token keyword"}},[t._v("return")]),t._v(" x\n"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("}")]),t._v("\n")])])])])}),[],!1,null,null,null);s.default=e.exports}}]);